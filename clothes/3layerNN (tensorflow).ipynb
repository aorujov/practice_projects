{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clothes\n",
    "\n",
    "### Data Format: (examples, features)\n",
    "\n",
    "* Input (60000, 28, 28)\n",
    "* Output (60000,)\n",
    "* X_train  -- (60000,28,28)\n",
    "* X_test  -- (10000,28,28)\n",
    "* Y_train  -- (60000,)\n",
    "* Y_test  -- (10000,)\n",
    "\n",
    "\n",
    "\n",
    "### Model\n",
    "* Flatten(28x28=784) > 40 > 36 > 10\n",
    "\n",
    "\n",
    "### Results\n",
    "* Accuracy: 86%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abido\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Flatten\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label dict\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 140\n",
    "batch_size = 128\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders\n",
    "X = tf.placeholder(tf.float32, [None, 784]) \n",
    "Y = tf.placeholder(tf.int64, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model variables\n",
    "W1 = tf.get_variable(\"W1\", [784, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "b1 = tf.get_variable(\"b1\", [1,40], initializer = tf.zeros_initializer())\n",
    "W2 = tf.get_variable(\"W2\", [40, 36], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "b2 = tf.get_variable(\"b2\", [1,36], initializer = tf.zeros_initializer())\n",
    "W3 = tf.get_variable(\"W3\", [36, 10], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "b3 = tf.get_variable(\"b3\", [1,10], initializer = tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "A1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "A2 = tf.nn.relu(tf.matmul(A1, W2) + b2)\n",
    "Z3 = tf.matmul(A2, W3) + b3\n",
    "pred = tf.nn.softmax(Z3)  #output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = Z3, labels = Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 2.447988987\n",
      "Epoch: 0002 cost= 2.112422228\n",
      "Epoch: 0003 cost= 1.940403104\n",
      "Epoch: 0004 cost= 1.656764269\n",
      "Epoch: 0005 cost= 1.453332424\n",
      "Epoch: 0006 cost= 1.316480756\n",
      "Epoch: 0007 cost= 1.154565334\n",
      "Epoch: 0008 cost= 1.034634948\n",
      "Epoch: 0009 cost= 0.969977319\n",
      "Epoch: 0010 cost= 0.904099107\n",
      "Epoch: 0011 cost= 0.840211689\n",
      "Epoch: 0012 cost= 0.798770428\n",
      "Epoch: 0013 cost= 0.784394383\n",
      "Epoch: 0014 cost= 0.739822149\n",
      "Epoch: 0015 cost= 0.712229311\n",
      "Epoch: 0016 cost= 0.694476187\n",
      "Epoch: 0017 cost= 0.673216105\n",
      "Epoch: 0018 cost= 0.665198267\n",
      "Epoch: 0019 cost= 0.647533894\n",
      "Epoch: 0020 cost= 0.629984856\n",
      "Epoch: 0021 cost= 0.620663524\n",
      "Epoch: 0022 cost= 0.610460699\n",
      "Epoch: 0023 cost= 0.600820839\n",
      "Epoch: 0024 cost= 0.592593074\n",
      "Epoch: 0025 cost= 0.583439946\n",
      "Epoch: 0026 cost= 0.571831524\n",
      "Epoch: 0027 cost= 0.563333333\n",
      "Epoch: 0028 cost= 0.553734660\n",
      "Epoch: 0029 cost= 0.543067753\n",
      "Epoch: 0030 cost= 0.535347342\n",
      "Epoch: 0031 cost= 0.527010143\n",
      "Epoch: 0032 cost= 0.519759893\n",
      "Epoch: 0033 cost= 0.513427496\n",
      "Epoch: 0034 cost= 0.505680919\n",
      "Epoch: 0035 cost= 0.501819313\n",
      "Epoch: 0036 cost= 0.497216314\n",
      "Epoch: 0037 cost= 0.492811382\n",
      "Epoch: 0038 cost= 0.490020066\n",
      "Epoch: 0039 cost= 0.486853570\n",
      "Epoch: 0040 cost= 0.488899261\n",
      "Epoch: 0041 cost= 0.502785087\n",
      "Epoch: 0042 cost= 0.494876027\n",
      "Epoch: 0043 cost= 0.480364561\n",
      "Epoch: 0044 cost= 0.465922475\n",
      "Epoch: 0045 cost= 0.475367934\n",
      "Epoch: 0046 cost= 0.477174819\n",
      "Epoch: 0047 cost= 0.457422018\n",
      "Epoch: 0048 cost= 0.473447293\n",
      "Epoch: 0049 cost= 0.473073423\n",
      "Epoch: 0050 cost= 0.450465888\n",
      "Epoch: 0051 cost= 0.477485031\n",
      "Epoch: 0052 cost= 0.456355155\n",
      "Epoch: 0053 cost= 0.454824150\n",
      "Epoch: 0054 cost= 0.457384259\n",
      "Epoch: 0055 cost= 0.436277807\n",
      "Epoch: 0056 cost= 0.451908380\n",
      "Epoch: 0057 cost= 0.430908710\n",
      "Epoch: 0058 cost= 0.443704754\n",
      "Epoch: 0059 cost= 0.428470105\n",
      "Epoch: 0060 cost= 0.433459967\n",
      "Epoch: 0061 cost= 0.424986601\n",
      "Epoch: 0062 cost= 0.425374925\n",
      "Epoch: 0063 cost= 0.423406363\n",
      "Epoch: 0064 cost= 0.418886423\n",
      "Epoch: 0065 cost= 0.420133501\n",
      "Epoch: 0066 cost= 0.413232565\n",
      "Epoch: 0067 cost= 0.415388465\n",
      "Epoch: 0068 cost= 0.408812374\n",
      "Epoch: 0069 cost= 0.411523879\n",
      "Epoch: 0070 cost= 0.405265659\n",
      "Epoch: 0071 cost= 0.406529784\n",
      "Epoch: 0072 cost= 0.402395517\n",
      "Epoch: 0073 cost= 0.402560622\n",
      "Epoch: 0074 cost= 0.399683356\n",
      "Epoch: 0075 cost= 0.398146093\n",
      "Epoch: 0076 cost= 0.397000849\n",
      "Epoch: 0077 cost= 0.394284636\n",
      "Epoch: 0078 cost= 0.394144684\n",
      "Epoch: 0079 cost= 0.390886158\n",
      "Epoch: 0080 cost= 0.391028702\n",
      "Epoch: 0081 cost= 0.388413727\n",
      "Epoch: 0082 cost= 0.387462854\n",
      "Epoch: 0083 cost= 0.386260599\n",
      "Epoch: 0084 cost= 0.384047389\n",
      "Epoch: 0085 cost= 0.383583069\n",
      "Epoch: 0086 cost= 0.381373763\n",
      "Epoch: 0087 cost= 0.380535394\n",
      "Epoch: 0088 cost= 0.379200280\n",
      "Epoch: 0089 cost= 0.377530873\n",
      "Epoch: 0090 cost= 0.376770318\n",
      "Epoch: 0091 cost= 0.375076294\n",
      "Epoch: 0092 cost= 0.374065787\n",
      "Epoch: 0093 cost= 0.372977972\n",
      "Epoch: 0094 cost= 0.371711671\n",
      "Epoch: 0095 cost= 0.371402293\n",
      "Epoch: 0096 cost= 0.371688157\n",
      "Epoch: 0097 cost= 0.374753386\n",
      "Epoch: 0098 cost= 0.386209100\n",
      "Epoch: 0099 cost= 0.400101036\n",
      "Epoch: 0100 cost= 0.410332859\n",
      "Epoch: 0101 cost= 0.375330210\n",
      "Epoch: 0102 cost= 0.368573159\n",
      "Epoch: 0103 cost= 0.388858736\n",
      "Epoch: 0104 cost= 0.371356785\n",
      "Epoch: 0105 cost= 0.363554090\n",
      "Epoch: 0106 cost= 0.374921083\n",
      "Epoch: 0107 cost= 0.365088552\n",
      "Epoch: 0108 cost= 0.360818475\n",
      "Epoch: 0109 cost= 0.368201017\n",
      "Epoch: 0110 cost= 0.359039247\n",
      "Epoch: 0111 cost= 0.359787226\n",
      "Epoch: 0112 cost= 0.363062859\n",
      "Epoch: 0113 cost= 0.355421692\n",
      "Epoch: 0114 cost= 0.357998371\n",
      "Epoch: 0115 cost= 0.358255267\n",
      "Epoch: 0116 cost= 0.352635026\n",
      "Epoch: 0117 cost= 0.355117768\n",
      "Epoch: 0118 cost= 0.354862660\n",
      "Epoch: 0119 cost= 0.349958897\n",
      "Epoch: 0120 cost= 0.351895958\n",
      "Epoch: 0121 cost= 0.351624936\n",
      "Epoch: 0122 cost= 0.347212970\n",
      "Epoch: 0123 cost= 0.349011719\n",
      "Epoch: 0124 cost= 0.348368466\n",
      "Epoch: 0125 cost= 0.345311135\n",
      "Epoch: 0126 cost= 0.346003145\n",
      "Epoch: 0127 cost= 0.345931947\n",
      "Epoch: 0128 cost= 0.343050778\n",
      "Epoch: 0129 cost= 0.343209565\n",
      "Epoch: 0130 cost= 0.343506843\n",
      "Epoch: 0131 cost= 0.341199815\n",
      "Epoch: 0132 cost= 0.340495825\n",
      "Epoch: 0133 cost= 0.340728372\n",
      "Epoch: 0134 cost= 0.339389354\n",
      "Epoch: 0135 cost= 0.338087589\n",
      "Epoch: 0136 cost= 0.337841600\n",
      "Epoch: 0137 cost= 0.337392330\n",
      "Epoch: 0138 cost= 0.336095631\n",
      "Epoch: 0139 cost= 0.335339457\n",
      "Epoch: 0140 cost= 0.334973246\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.8594\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "   \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={X: X_train, Y: Y_train})\n",
    "        \n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), Y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
